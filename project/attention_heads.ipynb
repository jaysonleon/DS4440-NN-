{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle\n",
    "import pickle\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# PyTorch\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "# SkLearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>2326</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>934 8616\\r\\ni got a missed call from yo bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15914</th>\n",
       "      <td>16283</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @KINGTUNCHI_: Fucking with a bad bitch you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18943</th>\n",
       "      <td>19362</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @eanahS__: @1inkkofrosess lol my credit ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16407</th>\n",
       "      <td>16780</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Maxin_Betha Wipe the cum out of them faggo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>13654</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Niggas cheat on they bitch and don't expect no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>2710</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@Bombfantasyyy @Angela_Mastr lmao angela you l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13037</th>\n",
       "      <td>13357</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My favorite is #picslip that's a dumb bitch, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>6153</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@freddurst shut up faggot, no one cares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16621</th>\n",
       "      <td>17004</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Neonte: We was wearing shorts about a week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23374</th>\n",
       "      <td>23860</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>You little twats.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "2281         2326      3            0                   3        0      1   \n",
       "15914       16283      3            0                   3        0      1   \n",
       "18943       19362      3            0                   1        2      2   \n",
       "16407       16780      3            0                   3        0      1   \n",
       "13326       13654      3            1                   2        0      1   \n",
       "...           ...    ...          ...                 ...      ...    ...   \n",
       "2647         2710      3            0                   3        0      1   \n",
       "13037       13357      3            0                   3        0      1   \n",
       "5982         6153      3            1                   2        0      1   \n",
       "16621       17004      3            0                   3        0      1   \n",
       "23374       23860      3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \n",
       "2281       934 8616\\r\\ni got a missed call from yo bitch  \n",
       "15914  RT @KINGTUNCHI_: Fucking with a bad bitch you ...  \n",
       "18943  RT @eanahS__: @1inkkofrosess lol my credit ain...  \n",
       "16407  RT @Maxin_Betha Wipe the cum out of them faggo...  \n",
       "13326  Niggas cheat on they bitch and don't expect no...  \n",
       "...                                                  ...  \n",
       "2647   @Bombfantasyyy @Angela_Mastr lmao angela you l...  \n",
       "13037  My favorite is #picslip that's a dumb bitch, w...  \n",
       "5982             @freddurst shut up faggot, no one cares  \n",
       "16621  RT @Neonte: We was wearing shorts about a week...  \n",
       "23374                                  You little twats.  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Datasets \n",
    "# Hate Xplain\n",
    "hate_xplain = pd.read_csv(r'project\\data\\hate_xplain.csv')\n",
    "\n",
    "# Implicit Hate \n",
    "implicit_hate = pd.read_csv(r'project\\data\\implicit-hate-corpus\\implicit_hate_v1_stg2_posts.tsv', delimiter='\\t')\n",
    "label_map = {\n",
    "    'white_grievance': 0, 'incitement': 1, 'inferiority': 2,\n",
    "    'irony': 3, 'stereotypical': 4, 'threatening': 5, 'other': 6\n",
    "}\n",
    "implicit_hate['class_label'] = implicit_hate['implicit_class'].map(label_map)\n",
    "implicit_hate.drop(\"extra_implicit_class\", axis=1, inplace=True)\n",
    "\n",
    "# Toxic-Spans\n",
    "annotations = pd.read_csv(r'project\\data\\toxic-spans\\annotations.csv')\n",
    "comments = pd.read_csv(r'project\\data\\toxic-spans\\comments.csv')\n",
    "toxic_spans = pd.merge(annotations, comments, on='comment_id')\n",
    "\n",
    "hate_xplain = hate_xplain.sample(n=1000, random_state=42)\n",
    "hate_xplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = pickle.load(open(r'project\\BERT\\bert.pkl', 'rb'))\n",
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_description.txt', 'w') as f:\n",
    "    f.write(str(bert_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer, max_length):\n",
    "    if isinstance(texts, pd.Series):\n",
    "        texts = texts.tolist()\n",
    "    texts = [str(text) for text in texts] \n",
    "\n",
    "    if isinstance(labels, pd.Series):\n",
    "        labels = labels.tolist()\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
    "    dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], labels)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 13)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hx_train_text, hx_test_text, hx_train_labels, hx_test_labels = train_test_split(hate_xplain['tweet'], hate_xplain['class'], test_size=0.2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=True)\n",
    "hx_train = tokenize_data(hx_train_text, hx_train_labels, tokenizer, 512)\n",
    "hx_test = tokenize_data(hx_test_text, hx_test_labels, tokenizer, 512)\n",
    "\n",
    "hx_train_loader = DataLoader(hx_train, batch_size=16, shuffle=True)\n",
    "hx_test_loader = DataLoader(hx_test, batch_size=16, shuffle=True)\n",
    "\n",
    "hx_train_loader.__len__(), hx_test_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_evaluation(model, data_loader, device=None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "hx_preds, hx_labels = baseline_evaluation(bert_model, hx_test_loader, device)\n",
    "classification_report(hx_labels, hx_preds, target_names=hate_xplain['class'].unique(), output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_zero_out_heads(model, data_loader, zeroed_heads=None, device=None):\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def zero_out_attention_head(module, input, output): \n",
    "        attention_probs = output[0]\n",
    "        for head_idx in zeroed_heads.get(layer_idx, []):\n",
    "            attention_probs[:, head_idx, :, :] = 0\n",
    "        return clear_output\n",
    "    \n",
    "    hooks = []\n",
    "    for layer_idx, heads in (zeroed_heads or {}).items():\n",
    "        layer = model.bert.encoder.layer[layer_idx].attention.self\n",
    "        hooks.append(layer.register_forward_hook(zero_out_attention_head))\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in data_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.eval()\n",
    "for i, (data, label) in enumerate(hx_test_loader):\n",
    "    if device == 'cuda':\n",
    "        data, label = data.to(device), label.to(device)\n",
    "    with torch.no_grad():\n",
    "        attention_heads = bert_model.bert.encoder.layer[0].attention.self\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSdpaSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_xplain.iloc[0]\n",
    "bert_model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(hate_xplain.iloc[0]['tweet'], return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "attention_heads = bert_model.bert.encoder.layer[0].attention.self\n",
    "attention_heads\n",
    "# for i in inputs:\n",
    "#     print(i, inputs[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_heads.query)\n",
    "print(attention_heads.key)\n",
    "print(attention_heads.value)\n",
    "print(attention_heads.dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
